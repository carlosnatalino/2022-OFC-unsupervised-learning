{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "base_alpha = .4\n",
    "alpha_diff = .6\n",
    "\n",
    "# mpl.rcParams['font.size'] = 22\n",
    "mpl.rcParams['grid.linestyle'] = ':'\n",
    "mpl.rcParams['lines.markersize'] = 8\n",
    "# font = fm.FontProperties(\n",
    "#         family = 'Cambria', fname = 'C:\\\\Windows\\\\Fonts\\\\Cambria.ttf')\n",
    "\n",
    "figure_format = 'svg'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "folder = '/nobackup/carda/datasets/2019-ecoc-demo'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open(os.path.join(folder, 'scaled_final_dataset.h5'), 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "    scaled_final_dataframe = dataset['scaled_final_dataframe']\n",
    "    class_names = dataset['class_names']\n",
    "    class_columns = dataset['class_columns']\n",
    "    del dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "number_samples_attack = []\n",
    "for ida, attack in enumerate(class_names):\n",
    "    number_samples_attack.append(scaled_final_dataframe[(scaled_final_dataframe['attack'] == ida)].shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "num_tests = 50\n",
    "samples_normal_vec = [30, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 400, 500]\n",
    "samples_abnormal = 30\n",
    "\n",
    "epsilon_configurations = [.1, .5, 1., 1.5, 2., 3., 4., 5., 10.]\n",
    "min_samples_configurations = [3, 5, 8, 10, 12, 15, 20]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "performance_dbscan = {}\n",
    "highest_f1 = {}\n",
    "\n",
    "for ids, samples_normal in enumerate(samples_normal_vec):\n",
    "\n",
    "    performance_dbscan[samples_normal] = {}\n",
    "    highest_f1[samples_normal] = .0\n",
    "\n",
    "    # generating the dictionary\n",
    "    for idm, min_samples in enumerate(min_samples_configurations):\n",
    "        performance_dbscan[samples_normal][min_samples] = {}\n",
    "        for ide, epsilon in enumerate(epsilon_configurations):\n",
    "            performance_dbscan[samples_normal][min_samples][epsilon] = {}\n",
    "\n",
    "    for idm, min_samples in enumerate(min_samples_configurations):\n",
    "        \n",
    "        for ide, epsilon in enumerate(epsilon_configurations):\n",
    "\n",
    "            clf = DBSCAN(eps=epsilon, min_samples=min_samples, metric='euclidean')\n",
    "            \n",
    "            true_positives = []\n",
    "            false_negatives = []\n",
    "            true_negatives = []\n",
    "            false_positives = []\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "            f1_scores = []\n",
    "            num_centroids = 0\n",
    "            \n",
    "            for test in range(num_tests):\n",
    "                start_index = np.random.randint(number_samples_attack[0]-samples_normal)\n",
    "                p = scaled_final_dataframe[(scaled_final_dataframe['attack'] == 0)]\n",
    "                x_data = p.values[start_index:start_index+samples_normal, :-1]\n",
    "                y_data = p.values[start_index:start_index+samples_normal, -1]\n",
    "\n",
    "                # start copy\n",
    "                for i in range(num_tests):\n",
    "                    x_data_test = np.copy(x_data)\n",
    "                    y_data_test = np.copy(y_data)\n",
    "                    for clazz in range(1, len(class_names)):\n",
    "                        start_index = np.random.randint(number_samples_attack[clazz]-samples_abnormal)\n",
    "                        x = scaled_final_dataframe[(scaled_final_dataframe['attack'] == clazz)]\n",
    "                        x_data_test = np.concatenate((x_data_test, x.values[start_index:start_index+samples_abnormal, :-1]), axis=0)\n",
    "                        y_data_test = np.concatenate((y_data_test, x.values[start_index:start_index+samples_abnormal, -1]), axis=0)\n",
    "\n",
    "                    y_pred = clf.fit_predict(x_data_test)\n",
    "    #                 y_pred = dbscan(x_data_test, epsilon, min_samples)\n",
    "    #                 print(y_pred, '\\n')\n",
    "                    \n",
    "    #                 num_centroids += len(clf.core_sample_indices_)\n",
    "\n",
    "                    tpr = np.sum([1 for i in range(len(y_pred)) if y_pred[i] == -1 and y_data_test[i] > 0]) / np.sum([1 for i in range(len(y_pred)) if y_data_test[i] > 0])\n",
    "                    fnr = np.sum([1 for i in range(len(y_pred)) if y_pred[i] >= 0 and y_data_test[i] > 0]) / np.sum([1 for i in range(len(y_pred)) if y_data_test[i] > 0])\n",
    "\n",
    "                    tnr = np.sum([1 for i in range(len(y_pred)) if y_pred[i] >= 0 and y_data_test[i] == 0]) / np.sum([1 for i in range(len(y_pred)) if y_data_test[i] == 0])\n",
    "                    fpr = np.sum([1 for i in range(len(y_pred)) if y_pred[i] == -1 and y_data_test[i] == 0]) / np.sum([1 for i in range(len(y_pred)) if y_data_test[i] == 0])\n",
    "\n",
    "                    if tpr == 0: # avoids division by zero\n",
    "                        precision = 0.\n",
    "                        recall = 0.\n",
    "                        f1 = 0.\n",
    "                    else:\n",
    "                        precision = tpr / (tpr + fpr)\n",
    "                        recall = tpr / (tpr + fnr)\n",
    "                        f1 = 2 * precision * recall / (precision + recall)\n",
    "                    \n",
    "                    true_positives.append(tpr)\n",
    "                    false_negatives.append(fnr)\n",
    "                    true_negatives.append(tnr)\n",
    "                    false_positives.append(fpr)\n",
    "                    precisions.append(precision)\n",
    "                    recalls.append(recall)\n",
    "                    f1_scores.append(f1)\n",
    "            \n",
    "            performance_dbscan[samples_normal][min_samples][epsilon]['min_samples'] = min_samples\n",
    "            performance_dbscan[samples_normal][min_samples][epsilon]['epsilon'] = epsilon\n",
    "            performance_dbscan[samples_normal][min_samples][epsilon]['true_positive_rate'] = np.mean(true_positives)\n",
    "            performance_dbscan[samples_normal][min_samples][epsilon]['false_negative_rate'] = np.mean(false_negatives)\n",
    "            performance_dbscan[samples_normal][min_samples][epsilon]['true_negative_rate'] = np.mean(true_negatives)\n",
    "            performance_dbscan[samples_normal][min_samples][epsilon]['false_positive_rate'] = np.mean(false_positives)\n",
    "            performance_dbscan[samples_normal][min_samples][epsilon]['precision'] = np.mean(precisions)\n",
    "            performance_dbscan[samples_normal][min_samples][epsilon]['recall'] = np.mean(recalls)\n",
    "            performance_dbscan[samples_normal][min_samples][epsilon]['f1_score'] = np.mean(f1_scores)\n",
    "            msg = ''\n",
    "            if performance_dbscan[samples_normal][min_samples][epsilon]['f1_score'] > highest_f1[samples_normal]:\n",
    "                msg = '\\t * highest'\n",
    "                highest_f1[samples_normal] = performance_dbscan[samples_normal][min_samples][epsilon]['f1_score']\n",
    "                print(f'{samples_normal:>4}\\t{min_samples:>6}\\t', f'{epsilon:>6}', '\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t'.format(performance_dbscan[samples_normal][min_samples][epsilon]['false_positive_rate'],\n",
    "                    performance_dbscan[samples_normal][min_samples][epsilon]['false_negative_rate'],\n",
    "                    performance_dbscan[samples_normal][min_samples][epsilon]['f1_score']), msg)\n",
    "\n",
    "    print(ids+1, ' / ', len(samples_normal_vec))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  30\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      "  30\t     5\t    0.5 \t0.910\t0.024\t0.678\t \t * highest\n",
      "  30\t    10\t    1.0 \t0.615\t0.155\t0.691\t \t * highest\n",
      "  30\t    12\t    1.0 \t0.658\t0.098\t0.711\t \t * highest\n",
      "1  /  14\n",
      "  50\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      "  50\t     3\t    0.5 \t0.651\t0.107\t0.704\t \t * highest\n",
      "  50\t     8\t    1.0 \t0.367\t0.240\t0.714\t \t * highest\n",
      "  50\t    10\t    1.0 \t0.487\t0.153\t0.729\t \t * highest\n",
      "  50\t    12\t    1.0 \t0.517\t0.099\t0.750\t \t * highest\n",
      "2  /  14\n",
      "  75\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      "  75\t     3\t    0.5 \t0.584\t0.106\t0.724\t \t * highest\n",
      "  75\t     8\t    1.0 \t0.261\t0.244\t0.750\t \t * highest\n",
      "  75\t    10\t    1.0 \t0.369\t0.162\t0.761\t \t * highest\n",
      "  75\t    12\t    1.0 \t0.415\t0.101\t0.781\t \t * highest\n",
      "3  /  14\n",
      " 100\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 100\t     3\t    0.5 \t0.557\t0.106\t0.731\t \t * highest\n",
      " 100\t     8\t    1.0 \t0.213\t0.247\t0.765\t \t * highest\n",
      " 100\t    10\t    1.0 \t0.249\t0.160\t0.804\t \t * highest\n",
      " 100\t    12\t    1.0 \t0.310\t0.112\t0.808\t \t * highest\n",
      "4  /  14\n",
      " 125\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 125\t     3\t    0.5 \t0.511\t0.108\t0.744\t \t * highest\n",
      " 125\t     8\t    1.0 \t0.216\t0.250\t0.761\t \t * highest\n",
      " 125\t    10\t    1.0 \t0.249\t0.168\t0.800\t \t * highest\n",
      " 125\t    12\t    1.0 \t0.271\t0.114\t0.823\t \t * highest\n",
      " 125\t    15\t    1.0 \t0.330\t0.067\t0.827\t \t * highest\n",
      "5  /  14\n",
      " 150\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 150\t     3\t    0.5 \t0.475\t0.105\t0.756\t \t * highest\n",
      " 150\t     8\t    1.0 \t0.173\t0.256\t0.774\t \t * highest\n",
      " 150\t    10\t    1.0 \t0.208\t0.177\t0.809\t \t * highest\n",
      " 150\t    12\t    1.0 \t0.254\t0.120\t0.824\t \t * highest\n",
      " 150\t    15\t    1.0 \t0.280\t0.075\t0.840\t \t * highest\n",
      "6  /  14\n",
      " 175\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 175\t     3\t    0.5 \t0.471\t0.105\t0.757\t \t * highest\n",
      " 175\t     8\t    1.0 \t0.157\t0.253\t0.782\t \t * highest\n",
      " 175\t    10\t    1.0 \t0.193\t0.177\t0.815\t \t * highest\n",
      " 175\t    12\t    1.0 \t0.242\t0.122\t0.828\t \t * highest\n",
      " 175\t    15\t    1.0 \t0.251\t0.067\t0.855\t \t * highest\n",
      "7  /  14\n",
      " 200\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 200\t     3\t    0.5 \t0.483\t0.107\t0.754\t \t * highest\n",
      " 200\t     8\t    1.0 \t0.137\t0.256\t0.788\t \t * highest\n",
      " 200\t    10\t    1.0 \t0.164\t0.182\t0.823\t \t * highest\n",
      " 200\t    12\t    1.0 \t0.222\t0.113\t0.841\t \t * highest\n",
      "8  /  14\n",
      " 225\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 225\t     3\t    0.5 \t0.443\t0.107\t0.766\t \t * highest\n",
      " 225\t     8\t    1.0 \t0.141\t0.256\t0.787\t \t * highest\n",
      " 225\t    10\t    1.0 \t0.172\t0.181\t0.821\t \t * highest\n",
      " 225\t    12\t    1.0 \t0.185\t0.127\t0.847\t \t * highest\n",
      " 225\t    20\t    1.0 \t0.276\t0.047\t0.858\t \t * highest\n",
      "9  /  14\n",
      " 250\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 250\t     3\t    0.5 \t0.450\t0.106\t0.764\t \t * highest\n",
      " 250\t     5\t    0.5 \t0.581\t0.023\t0.766\t \t * highest\n",
      " 250\t     8\t    1.0 \t0.127\t0.260\t0.789\t \t * highest\n",
      " 250\t    10\t    1.0 \t0.156\t0.180\t0.828\t \t * highest\n",
      " 250\t    12\t    1.0 \t0.197\t0.136\t0.838\t \t * highest\n",
      " 250\t    15\t    1.0 \t0.203\t0.090\t0.861\t \t * highest\n",
      "10  /  14\n",
      " 275\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 275\t     3\t    0.5 \t0.400\t0.106\t0.780\t \t * highest\n",
      " 275\t     8\t    1.0 \t0.124\t0.259\t0.792\t \t * highest\n",
      " 275\t    10\t    1.0 \t0.134\t0.188\t0.832\t \t * highest\n",
      " 275\t    12\t    1.0 \t0.181\t0.123\t0.850\t \t * highest\n",
      " 275\t    15\t    1.0 \t0.208\t0.083\t0.863\t \t * highest\n",
      "11  /  14\n",
      " 300\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 300\t     3\t    0.5 \t0.421\t0.108\t0.772\t \t * highest\n",
      " 300\t     5\t    0.5 \t0.511\t0.025\t0.787\t \t * highest\n",
      " 300\t     8\t    1.0 \t0.106\t0.258\t0.799\t \t * highest\n",
      " 300\t    10\t    1.0 \t0.133\t0.194\t0.829\t \t * highest\n",
      " 300\t    12\t    1.0 \t0.162\t0.128\t0.855\t \t * highest\n",
      " 300\t    15\t    1.0 \t0.177\t0.087\t0.873\t \t * highest\n",
      "12  /  14\n",
      " 400\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 400\t     3\t    0.5 \t0.379\t0.111\t0.784\t \t * highest\n",
      " 400\t     5\t    0.5 \t0.516\t0.025\t0.784\t \t * highest\n",
      " 400\t     8\t    1.0 \t0.094\t0.266\t0.799\t \t * highest\n",
      " 400\t    10\t    1.0 \t0.113\t0.194\t0.837\t \t * highest\n",
      " 400\t    12\t    1.0 \t0.126\t0.152\t0.857\t \t * highest\n",
      " 400\t    15\t    1.0 \t0.169\t0.096\t0.871\t \t * highest\n",
      "13  /  14\n",
      " 500\t     3\t    0.1 \t1.000\t0.000\t0.667\t \t * highest\n",
      " 500\t     3\t    0.5 \t0.362\t0.109\t0.791\t \t * highest\n",
      " 500\t     5\t    0.5 \t0.489\t0.026\t0.792\t \t * highest\n",
      " 500\t     8\t    1.0 \t0.080\t0.267\t0.804\t \t * highest\n",
      " 500\t    10\t    1.0 \t0.095\t0.201\t0.840\t \t * highest\n",
      " 500\t    12\t    1.0 \t0.127\t0.142\t0.862\t \t * highest\n",
      " 500\t    15\t    1.0 \t0.153\t0.102\t0.874\t \t * highest\n",
      "14  /  14\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "with open('../results/results-dbscan.h5', 'wb') as file:\n",
    "    pickle.dump((performance_dbscan, highest_f1), file)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "9b167658ac90bced5f0c9629166cf1cfe8752ee37fcc8c1eec9a3ec66ab03655"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}